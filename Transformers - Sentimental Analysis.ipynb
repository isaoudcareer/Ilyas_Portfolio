{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom sklearn import preprocessing\nfrom tokenizers import BertWordPieceTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-12-02T22:24:35.077424Z","iopub.execute_input":"2022-12-02T22:24:35.078361Z","iopub.status.idle":"2022-12-02T22:24:42.623362Z","shell.execute_reply.started":"2022-12-02T22:24:35.078241Z","shell.execute_reply":"2022-12-02T22:24:42.622580Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2022-12-02 22:24:36.852109: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-12-02 22:24:36.852253: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"#USING TPU \n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T22:24:42.624902Z","iopub.execute_input":"2022-12-02T22:24:42.625163Z","iopub.status.idle":"2022-12-02T22:24:49.300602Z","shell.execute_reply.started":"2022-12-02T22:24:42.625130Z","shell.execute_reply":"2022-12-02T22:24:49.299620Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\n","output_type":"stream"},{"name":"stderr","text":"2022-12-02 22:24:42.633112: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-12-02 22:24:42.637015: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-12-02 22:24:42.637055: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2022-12-02 22:24:42.637083: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (13a7ba5d3af9): /proc/driver/nvidia/version does not exist\n2022-12-02 22:24:42.640800: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-02 22:24:42.642193: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-12-02 22:24:42.675896: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-12-02 22:24:42.675985: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2022-12-02 22:24:42.693925: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-12-02 22:24:42.693979: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2022-12-02 22:24:42.695690: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30020\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"def BERT_MODEL(CSV,text_col,target_col,EPOCHS, BATCH_SIZE, MAX_LEN, NUM_TARGET_VALUES):\n\n    data= pd.read_csv(CSV)\n    \n    if data[target_col].dtypes != 'int64':\n        le = preprocessing.LabelEncoder()\n        data[target_col] = le.fit_transform(data[target_col])\n    \n    X = data[text_col].values\n    y = data[target_col].values\n\n    X_train,X_test,y_train,y_test = train_test_split(X,y,\n                                                    test_size = 0.25,\n                                                    random_state=0\n                                                    )\n    \n    def fast_encode(texts, tokenizer, chunk_size=256, maxlen=MAX_LEN):\n\n        tokenizer.enable_truncation(max_length=maxlen)\n        tokenizer.enable_padding(length=maxlen)\n        all_ids = []\n\n        for i in tqdm(range(0, len(texts), chunk_size)):\n            text_chunk = texts[i:i+chunk_size].tolist()\n            encs = tokenizer.encode_batch(text_chunk)\n            all_ids.extend([enc.ids for enc in encs])\n\n        return np.array(all_ids)\n\n    #IMP DATA FOR CONFIG\n\n    AUTO = tf.data.experimental.AUTOTUNE\n\n\n    # First load the real tokenizer\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n    # Save the loaded tokenizer locally\n    tokenizer.save_pretrained('.')\n    # Reload it with the huggingface tokenizers library\n    fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n    fast_tokenizer\n\n    x_train = fast_encode(X_train.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n    x_valid = fast_encode(X_test.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\n    y_train = y_train\n    y_valid = y_test\n\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_train, y_train))\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n\n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_valid, y_valid))\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)\n    )\n\n    def build_model(transformer, max_len, num_outputs): \n        \"\"\"\n        function for training the BERT model\n        \"\"\"\n        input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        sequence_output = transformer(input_word_ids)[0]\n        cls_token = sequence_output[:, 0, :]\n        out = Dense(num_outputs , activation='softmax')(cls_token)\n\n        model = Model(inputs=input_word_ids, outputs=out)\n        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n        return model\n\n    with strategy.scope(): #Remove strategy.scope() if not using the TPU and leave the 'transformer_layer' and 'model' with the main function.\n        transformer_layer = (\n            transformers.TFDistilBertModel\n            .from_pretrained('distilbert-base-multilingual-cased')\n        )\n        model = build_model(transformer_layer, max_len=MAX_LEN, num_outputs=NUM_TARGET_VALUES )\n        model.summary()\n\n    n_steps = x_train.shape[0] // BATCH_SIZE\n    train_history = model.fit(\n        train_dataset,\n        steps_per_epoch=n_steps,\n        validation_data=valid_dataset,\n        epochs=EPOCHS\n    )\n    return(train_history)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T22:24:49.303152Z","iopub.execute_input":"2022-12-02T22:24:49.303401Z","iopub.status.idle":"2022-12-02T22:24:49.319228Z","shell.execute_reply.started":"2022-12-02T22:24:49.303373Z","shell.execute_reply":"2022-12-02T22:24:49.318453Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Replace the configurations below before running the code. \n\nNUM_TARGET_VALUES = 3 #Target column of my data has 3 unique target values \nEPOCHS = 10 \nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 200 # This will depend on the length of the input that has the max number of words or tokens\nCSV = \"/kaggle/input/preprocessed-dataset-sentiment-analysis/EcoPreprocessed.csv\" # CSV location of your data\ntext_col = \"review\" #column name where text is stored \ntarget_col = \"division\" #column name where target is stored\n\nBERT_MODEL(CSV,text_col,target_col,EPOCHS, BATCH_SIZE, MAX_LEN, NUM_TARGET_VALUES)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T22:24:49.320463Z","iopub.execute_input":"2022-12-02T22:24:49.320743Z","iopub.status.idle":"2022-12-02T22:27:05.015389Z","shell.execute_reply.started":"2022-12-02T22:24:49.320691Z","shell.execute_reply":"2022-12-02T22:27:05.013160Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c1d274a4114da1aa70e1a467edc774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a29d32198f554edda24d02e9ee6ad6db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307848c5eb9f4510aa5dc023aca22c79"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 12/12 [00:00<00:00, 89.63it/s]\n100%|██████████| 4/4 [00:00<00:00, 116.03it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbd35a88290145479e577e7800b4ef5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/911M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8358643f25654446bca1305e6ab96635"}},"metadata":{}},{"name":"stderr","text":"2022-12-02 22:25:22.378751: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\nSome layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 200)]             0         \n_________________________________________________________________\ntf_distil_bert_model (TFDist TFBaseModelOutput(last_hi 134734080 \n_________________________________________________________________\ntf.__operators__.getitem (Sl (None, 768)               0         \n_________________________________________________________________\ndense (Dense)                (None, 3)                 2307      \n=================================================================\nTotal params: 134,736,387\nTrainable params: 134,736,387\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n23/23 [==============================] - 50s 434ms/step - loss: 0.8032 - accuracy: 0.6599 - val_loss: 0.7041 - val_accuracy: 0.7581\nEpoch 2/10\n23/23 [==============================] - 3s 126ms/step - loss: 0.7423 - accuracy: 0.7403 - val_loss: 0.6849 - val_accuracy: 0.7581\nEpoch 3/10\n23/23 [==============================] - 3s 126ms/step - loss: 0.6817 - accuracy: 0.7517 - val_loss: 0.5363 - val_accuracy: 0.7728\nEpoch 4/10\n23/23 [==============================] - 3s 129ms/step - loss: 0.5462 - accuracy: 0.7687 - val_loss: 0.4429 - val_accuracy: 0.8002\nEpoch 5/10\n23/23 [==============================] - 3s 129ms/step - loss: 0.4364 - accuracy: 0.8091 - val_loss: 0.4202 - val_accuracy: 0.8247\nEpoch 6/10\n23/23 [==============================] - 3s 129ms/step - loss: 0.3987 - accuracy: 0.8335 - val_loss: 0.3728 - val_accuracy: 0.8433\nEpoch 7/10\n23/23 [==============================] - 3s 125ms/step - loss: 0.3313 - accuracy: 0.8718 - val_loss: 0.3624 - val_accuracy: 0.8580\nEpoch 8/10\n23/23 [==============================] - 3s 127ms/step - loss: 0.2832 - accuracy: 0.8888 - val_loss: 0.3235 - val_accuracy: 0.8746\nEpoch 9/10\n23/23 [==============================] - 3s 127ms/step - loss: 0.2376 - accuracy: 0.9057 - val_loss: 0.3345 - val_accuracy: 0.8697\nEpoch 10/10\n23/23 [==============================] - 3s 128ms/step - loss: 0.1936 - accuracy: 0.9301 - val_loss: 0.3281 - val_accuracy: 0.8825\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f815e3dbe50>"},"metadata":{}}]}]}